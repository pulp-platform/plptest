#!/usr/bin/env python3


#
# Copyright (C) 2018 ETH Zurich and University of Bologna
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


import argparse
import os
import logging
try:
  import psutil
except:
  pass
import csv
import collections
import datetime

# In case gtk3 is not available just skip gui
try:
  from plptest_gui import *
except:
  pass

from plptest_runner import *
import openpyxl



if os.environ.get('PYTHON_LOG') != None:
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


class Worktable():

    def __init__(self, name, parent, ws, first_row, last_row, first_column, last_column, commit):
        self.name = name
        self.first_row = first_row
        self.last_row = last_row
        self.first_column = first_column
        self.last_column = last_column
        self.ws = ws
        self.parent = parent
        self.commit = commit

    def add(self, score):
        row = [self.commit, datetime.now(), score, '']
        for i in range(0, len(row)):
            self.ws.cell(row=self.last_row+1, column=self.first_column+i, value=row[i])

        self.last_row += 1

    def __str__(self):
        return f'{self.first_row} -> {self.last_row}, {self.first_column} -> {self.last_column}'

class Worksheet():

    def __init__(self, name, ws, commit):
        self.name = name
        self.tables = collections.OrderedDict({})
        self.ws = ws
        self.tables_row = 1
        self.commit = commit

        tables_row = None
        for i in range(1, ws.max_row + 1):
            value = ws[f'A{i}'].value
            if value is not None:
                tables_row = i
                break

        if tables_row is not None:
            self.tables_row = tables_row
            table_name = None

            for i in range(1, ws.max_column + 1):

                value = ws.cell(row=tables_row+1, column=i).value

                if table_name is None and value is not None:
                    table_name = value
                    first_column = i
                    continue

                if table_name is not None and (value is None or i == ws.max_column):

                    table_name = None
                    last_column = i - 1 if value is None else i

                    for j in range(tables_row, ws.max_row + 1):

                        value = ws.cell(row=j, column=first_column).value

                        if table_name is None and value is not None:
                            table_name = value
                            continue

                        if table_name is not None and (value is None or j == ws.max_row):
                            last_row = j - 1 if value is None else j
                            table = Worktable(table_name, self, ws, tables_row, last_row, first_column, last_column, commit)
                            self.tables[table_name] = table
                            table_name = None
                            break

                    continue


    def get_table(self, name):
        if self.tables.get(name) is None:

            first_column = None

            for i in range(1, self.ws.max_column + 1):

                value = self.ws.cell(row=2, column=i).value

                if value is not None:
                    first_column = i

            if first_column is None:
                first_column = 1
            else:
                first_column += 2

            self.ws.cell(row=self.tables_row, column=first_column, value=name)

            rows = ['Commit', 'Date', 'Value', 'Comments']

            self.tables[name] = Worktable(name, self, self.ws,
                self.tables_row, self.tables_row + 1,
                first_column, first_column+len(rows)-1,
                self.commit)

            for i, cell_name in enumerate(rows):
                self.ws.cell(row=self.tables_row+1, column=first_column+i, value=cell_name)

        return self.tables.get(name)

    def __str__(self):
        result = ''
        for name, table in self.tables.items():
            result += f'Table {name}\n  '
            result += str(table)
            result += '\n'

        return result

class Workbook():

    def __init__(self, desc):

        info, filename = desc.split('@')
        sheetname, commit = info.split(':')

        self.wb = openpyxl.load_workbook(filename)
        self.sheets = {}
        self.filename = filename
        self.sheetname = sheetname

        for name in self.wb.sheetnames:
            self.sheets[name] = Worksheet(name, self.wb[name], commit)

    def get_sheet(self, name=None):
        if name is None:
            name = self.sheetname
        return self.sheets[name]

    def save(self):
        self.wb.save(filename=self.filename)

    def __str__(self):
        result = ''
        for name, sheet in self.sheets.items():
            result += f'Sheet {name}\n'
            result += str(sheet)
            result += '\n'
        return result

def run_done():
    runner.plpobjects.dumpTestsToConsole()
    if args.junit:
      runner.plpobjects.dumpTestsToJunit(reportPath + '/junit-reports')
    runner.dump_testplan()
    runner.command_done()

def run(tests):
  runner.runTests(args.testList, args.testList_re, run_done)
  return 0

def show(tests):
  for testset in tests:
    testset.show()

  runner.command_done()

  return 0

def score(tests):

  nb_score = 0
  score = 0
  error = False

  table = PrettyTable(['Test', 'Bench item', 'Desc', 'Value', 'score'])
  table.float_format='.3'
  table.align = "l"

  workbook = None
  if args.workbook is not None:
    workbook = Workbook(args.workbook)

  for testset in tests:
    (test_error, test_score, test_nb_score) = testset.score(table=table, score_name=args.score_name, workbook=workbook)

    error = error or test_error

    if test_nb_score > 0:
      score += test_score
      nb_score += test_nb_score

  if nb_score > 0:
      score = score / nb_score

  print (table)

  print ('\nFinal score: %f' % score)

  runner.command_done()

  if args.workbook is not None:
    workbook.save()

  return error

commands = {
  'run'    : ['Run the tests', run],
  'show'   : ['Show the tests', show],
  'score'   : ['Show the scores', score],
}
	

commandHelp = """Available commands:
"""

for name, cmd in commands.items():
	commandHelp += '  %-10s %s\n' % (name, cmd[0])

parser = argparse.ArgumentParser(description='Run a testset',epilog=commandHelp, formatter_class=argparse.RawDescriptionHelpFormatter)

parser.add_argument('command', metavar='CMD', type=str, nargs='*',
                   help='a command to be executed (see the command help afterwards)')

parser.add_argument("--config-def", dest="configDef", action="append", default=None, help="Specifies json files containing configurations definition")
parser.add_argument("--property", dest="properties", action="append", default=[], help="Specifies property")
parser.add_argument("--tag", dest="tags", action="append", default=[], help="Specifies tag")
parser.add_argument("--flags", dest="flags", action="append", default=[], help="Specifies flags")
parser.add_argument("--testset", dest="testset", action="append", default=None, metavar="PATH", help="Path to the testset. Default: %(default)s")
parser.add_argument("--cmd", dest="commands", action="append", default=None, metavar="PATH", help="Add command to be executed. Default: %(default)s")
parser.add_argument("--cmd-exclude", dest="commands_exclude", action="append", default=None, metavar="PATH", help="Add command to be excluded. Default: %(default)s")
parser.add_argument("--gui", dest="gui", action="store_true", help="Opens user interface")
parser.add_argument("--no-junit", dest="junit", action="store_false", help="Don't generate JUNIT report")
parser.add_argument("--no-fail", dest="no_fail", action="store_true", help="Return an error if there is any test failure")
parser.add_argument("--dry-run", dest="dry_run", action="store_true", help="Dry run")
parser.add_argument("--threads", dest="threads", default=None, type=int, help="Specify the number of worker threads")
parser.add_argument("--load-average", dest="load_average", default=0.9, type=float, help="Specify the system average load that this tool should try to respect, from 0 to 1")
parser.add_argument("--stdout", dest="stdout", action="store_true", help="Dumps test output to stdout")
parser.add_argument("--safe-stdout", dest="safe_stdout", action="store_true", help="Dumps test output to stdout once the test is done")
parser.add_argument("--max-output-len", dest="maxOutputLen", type=int, default=-1, help="Maximum length of a test output. Default: %(default)s bytes")
parser.add_argument("--test", dest="testList", default=None, action="append", help="Specify a test to be run")
parser.add_argument("--skip", dest="testSkipList", default=None, action="append", help="Specify a test to be skipped")
parser.add_argument("--test-re", dest="testList_re", default=None, action="append", help="Specify a test (based on a regular expression) to be run")
parser.add_argument("--max-timeout", dest="maxTimeout", default="3600", help="Sets maximum timeout allowed for a test")
parser.add_argument("--db", dest="db", action='store_true', default=False, help="Activate Pulp database access")
parser.add_argument("--worker-pool", dest="worker_pool", default=None, help="Specify worker pool")
parser.add_argument("--score-csv-file", dest="score_csv_file", default=None, help="Specify CSV file for scores")
parser.add_argument("--workbook", dest="workbook", default=None, help="Specify workbook")
parser.add_argument("--score-name", dest="score_name", default='score', help="Specify name for scores")
parser.add_argument("--config", dest="config", default='default', help="Specify config name")
parser.add_argument("--bench-csv-file", dest="bench_csv_file", default=None, help="Specify CSV file for benchmark results")
parser.add_argument("--job-id", dest="job_id", default=0, type=int, help="Give the job ID when the testset is split into several jobs")
parser.add_argument("--nb-jobs", dest="nb_jobs", default=1, type=int, help="Give the number of jobs when the testset is split into several jobs")
parser.add_argument("--bench-regexp", dest="bench_regexp", default='.*@BENCH@(.*)@DESC@(.*)@', help="Specify regexp for extracting benchmark results")


args = parser.parse_args()

if args.testset is None:
  args.testset = [os.getcwd() + '/testset.cfg']

if args.threads is None:
  try:
    args.threads = psutil.cpu_count(logical=True)
  except:
    args.threads = 1

reportPath = os.getcwd()

bench_csv_file = None
if args.bench_csv_file is not None:
  bench_csv_file = collections.OrderedDict()
  if os.path.exists(args.bench_csv_file):
    with open(args.bench_csv_file, 'r') as file:
      csv_reader = csv.reader(file)
      for row in csv_reader:
        bench_csv_file[row[0]] = row[1:]


def command_handle():
  if len(args.command) == 0:
    runner.stop()
    return

  cmd = args.command.pop()

  logging.debug('Executing command: ' + cmd)

  if commands.get(cmd) == None:
      raise Exception('Invalid command: ' + cmd)
  elif commands.get(cmd)[1](runner.tests) != 0:
    print ()
    print (bcolors.FAIL + 'FATAL ERROR: the command \'%s\' has failed' % (cmd) + bcolors.ENDC)
    exit(1)

if len(args.command) == 0 and not args.gui:
  args.command.append('run')

if os.environ.get('PLPTEST_DEFAULT_PROPERTIES') is not None:
  properties = []
  for prop in os.environ.get('PLPTEST_DEFAULT_PROPERTIES').split(' '):
    properties.append(prop)

  args.properties = properties + args.properties

runner = TestRunner(
    nbThreads=args.threads, stdout=args.stdout, safe_stdout=args.safe_stdout, maxOutputLen=args.maxOutputLen,
    maxTimeout=args.maxTimeout, worker_pool=args.worker_pool, db=args.db, average_load=args.load_average,
    bench_csv_file=bench_csv_file, bench_regexp=args.bench_regexp, commands=args.commands, exclude_commands=args.commands_exclude, dry_run=args.dry_run,
    server=args.gui, properties=args.properties, tags=args.tags, job_id=args.job_id % args.nb_jobs, nb_jobs=args.nb_jobs, flags=args.flags, skip_tests=args.testSkipList
)
for testset in args.testset:
  runner.addTestset(testset)


runner.addConfig(args.config)
# if args.config_name is None:
#   for config in plpconf.get_configs_from_env():
#     print (config)
#     runner.addConfig(config)
# else:
#   config = plpconf.get_config_from_tag(args.config_name)
#   runner.addConfig(config)



if args.gui:
  gui = TestGui()
  runner.start()
else:
  runner.start(command_handle)

if bench_csv_file is not None:
  with open(args.bench_csv_file, 'w') as file:
    csv_writer = csv.writer(file)
    for key, value in bench_csv_file.items():
      csv_writer.writerow([key] + value)

if args.no_fail:
  exit(runner.plpobjects.status())